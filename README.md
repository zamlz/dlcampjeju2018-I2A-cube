Imagination-Augmented Agents for Deep Reinforcement Learning to Solve Rubik's Cubes
===================================================================================
Jeju Deep Learning Camp 2018 [ Amlesh Sivanantham ]
---------------------------------------------------

To solve a Rubik's Cube environment with the model prescribed in the paper,
[Imagination-Augmented Agents for Deep Reinforcement Learning, \[*Weber et al*\]][i2a-paper].
The I2A model generates observation predictions from an learned environment model.
I2A's learn to leverage multiple rollouts of these predicitions to construct a
better policy and value network for the agent. 

Imagination-Augmented Agents (I2A) Details
------------------------------------------

#### Full I2A Model Architecture
![Full I2A Model Architecture][full-i2a]

The *Value and Policy Function* leverage information from a model-free and a 
model-based path. The model-free path simply processed the current observation
from the environment. The model-based path utilizes the observation to generate
multiple rollout encoding which are aggregated and fed to the rest of the
network. The rollout encodings are generated by the *Imagination Core* which
uses a learned *Environment Model* to create these rollouts. These encodings
can contain information about solving certain subproblems within the environment
that may not yeild any reward but is beneficial for getting reward later on.

#### Environment Model
![Environment Model][env-model]

The *Environment Model* is a simple Convolutional Network that takes in the
current observation of the state and action and outputs the predicted next state
and reward that the real environment would produce. The input action is converted
to a one-hot vector which then converted further into a one-hot channel
representation and stacked alongside the other channels of the input observation.
The accuracy of the model does improve the overall performance of the I2A model,
but it was shown in the paper that even with a bad model, the full architecture
is able to disregard the inaccuracies and still improve attain better performance
when compared to a totally model-free counterpart.

#### Imagination Core and Encoder
![Imagination Core and Encoder][imagine-core]

The *Imagination Core* is responsible for generating the trajectories and encoding
them. We utilize a rollout policy that makes the decision of choosing what
actions to take given either the real observation state or any imagined state
generated by the *Environment Model*. The choice of rollout policy is up for
experimentation and the details of it are described in the next section. The
rollout policy is used to generate n rollouts. Each rollout is then passed into
a convolutional Encoder and LSTM network which outputs the encoding for the rollout.

#### Rollout Policy
It turns out that for the environments the paper used, distilling the full I2A network
into a smaller and entirely model-free network showed the best results for being the
rollout policy. This is because the policy becomes goal-oriented and thus produces
rollouts that are searching for the goal.


The Environment
---------------
![Fake Solve Cube][cube-gif]

(Note that this is **NOT** the agent solving the environment)

The Rubik's Cube environment is actually a really simple environment that with
only 12 possible actions (not counting mid-turns). If we include the ability
to change orientation of the cube, we get a total of 18 actions. The 3x3x3
Rubik's Cube has **43,252,003,274,489,856,000 permutations** making it equivalent
to a very large maze. Out these permutations, there is only a single state that
yeilds reward, the solved state. Theoretically, regardless of how scrambled the
cube is, it can also be solved in 26 moves or less.

In our setup, the environment is setup as a gym environment using the API that
OpenAI's Gym Framework provides. The reward is made to be sparse and only provides
a reward of +1 on the solved state and provides a reward of +0 for all other states.
While the environment supports any n-order cube, testing will only be done of the
`cube-x2-v0` and `cube-x3-v0` environments. We will also explore how effective
the policy can become when allowing the agent to control the orientation of the
environment.



[i2a-paper]: https://arxiv.org/abs/1707.06203v2

[cube-gif]: https://raw.githubusercontent.com/zamlz/dlcampjeju2018-I2A-cube/master/docs/pics/cube_solve.gif
[env-model]: https://raw.githubusercontent.com/zamlz/dlcampjeju2018-I2A-cube/master/docs/pics/env_model.png
[full-i2a]: https://raw.githubusercontent.com/zamlz/dlcampjeju2018-I2A-cube/master/docs/pics/full_i2a.png
[imagine-core]: https://raw.githubusercontent.com/zamlz/dlcampjeju2018-I2A-cube/master/docs/pics/imagine.png
